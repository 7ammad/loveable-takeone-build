import { NextResponse } from 'next/server';
import type { NextRequest } from 'next/server';
import { redis } from '@/packages/core-lib/src/redis';
import { RateLimiterRedis } from 'rate-limiter-flexible';

// This middleware will run on all API routes
export const config = {
  matcher: '/api/:path*',
};

// Initialize rate limiter
const rateLimiter = redis ? new RateLimiterRedis({
  storeClient: redis,
  keyPrefix: 'ratelimit',
  points: 100, // 100 requests
  duration: 60, // per 60 seconds by IP
}) : null;

export async function middleware(request: NextRequest) {
  const { method, ip } = request;

  // --- Rate Limiting ---
  // Apply to all API routes for now. Can be made more granular.
  if (rateLimiter && ip) {
    try {
      await rateLimiter.consume(ip);
    } catch (rateLimiterRes) {
      return new NextResponse('Too Many Requests', { status: 429 });
    }
  }

  // --- Idempotency for POST/PUT/PATCH/DELETE requests ---
  if (['POST', 'PUT', 'PATCH', 'DELETE'].includes(method)) {
    const idempotencyKey = request.headers.get('Idempotency-Key');
    if (idempotencyKey) {
      // Middleware in Next.js runs at the edge and cannot modify the response
      // after it has been generated by the API route. Therefore, we can't
      // implement the full cache-on-write pattern here.
      // We will implement a simpler "locking" mechanism to prevent concurrent requests.
      
      const lockKey = `idempotency-lock:${idempotencyKey}`;
      const isLocked = await redis?.get(lockKey);

      if (isLocked) {
        // A request with this key is already in progress.
        return new NextResponse('A request with this Idempotency-Key is already in progress.', { status: 409 }); // 409 Conflict
      }

      // Set a lock with a short TTL to prevent race conditions.
      // The API route itself will be responsible for caching the actual response.
      await redis?.set(lockKey, 'locked', 'EX', 30); // 30-second lock
    }
  }

  return NextResponse.next();
}
